# -*- coding: utf-8 -*-
"""text_classification_machine_human_arabertv2_test_:(.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Dm2e4Lo7kvkUwnOA7QUT9HGCJNyQeDWc
"""

!pip install num2words
!pip install emoji

import re
import random
import numpy as np
import torch
import pandas as pd
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader, random_split
from transformers import AutoTokenizer, AutoModel
from torch.optim import AdamW
from transformers.optimization import get_linear_schedule_with_warmup
from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support
import warnings
warnings.filterwarnings('ignore')
from sklearn.model_selection import train_test_split
from num2words import num2words
import emoji

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
set_seed(42)

CONFIG = {
    'model_name': "aubmindlab/bert-base-arabertv2",
    'max_length': 256,
    'batch_size': 16,
    'learning_rate': 1e-5,  # Reduced to prevent overfitting
    'num_epochs': 15,
    'warmup_steps': 100,
    'weight_decay': 0.05,  # Increased for better regularization
    'dropout': 0.5,  # Increased dropout to prevent overfitting
    'early_stopping_patience': 2,  # Stop earlier to prevent overfitting
    'label_smoothing': 0.1,  # Add label smoothing
}

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

def normalize_arabic(text):
    text = re.sub("[إأآا]", "ا", text)
    text = re.sub("ى", "ي", text)
    text = re.sub("ؤ", "و", text)
    text = re.sub("ئ", "ي", text)
    text = re.sub("ة", "ه", text)
    return text

def remove_diacritics(text):
    arabic_diacritics = re.compile(r"[\u0617-\u061A\u064B-\u0652\u0670\u06D6-\u06ED]")
    return re.sub(arabic_diacritics, "", text)

def remove_tatweel(text):
    return re.sub(r'ـ+', '', text)

def normalize_whitespace(text):
    return re.sub(r'\s+', ' ', text).strip()

def clean_text(text):
    text = str(text).strip()
    text = normalize_arabic(text)
    text = remove_diacritics(text)
    text = remove_tatweel(text)
    text = normalize_whitespace(text)
    return text

def augment_text(text, p=0.5):
    """Enhanced data augmentation for Arabic text"""
    if random.random() > p:
        return text

    words = text.split()
    if len(words) < 3:
        return text

    aug_type = random.choice(['swap', 'shuffle', 'drop', 'none'])

    if aug_type == 'swap' and len(words) > 2:
        idx1, idx2 = random.sample(range(len(words)), 2)
        words[idx1], words[idx2] = words[idx2], words[idx1]
    elif aug_type == 'shuffle' and len(words) > 3:
        start = random.randint(0, len(words) - 3)
        end = start + random.randint(2, min(4, len(words) - start))
        segment = words[start:end]
        random.shuffle(segment)
        words[start:end] = segment
    elif aug_type == 'drop' and len(words) > 5:
        if random.random() < 0.3:
            words.pop(random.randint(0, len(words) - 1))

    return ' '.join(words)

class ArabicDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length, augment=False):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.augment = augment

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        if self.augment and random.random() > 0.5:
            text = augment_text(text)
        encoding = self.tokenizer(text, padding='max_length', truncation=True, max_length=self.max_length, return_tensors='pt')
        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
        }
        if self.labels is not None:
            item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)

        return item

class ImprovedBertClassifier(nn.Module):
    def __init__(self, model_name, num_labels=2, dropout=0.3):
        super().__init__()
        self.bert = AutoModel.from_pretrained(model_name)

        # Freeze some BERT layers to prevent overfitting
        # Freeze first 6 layers (out of 12)
        for param in list(self.bert.embeddings.parameters()):
            param.requires_grad = False
        for layer in self.bert.encoder.layer[:6]:
            for param in layer.parameters():
                param.requires_grad = False

        self.dropout1 = nn.Dropout(dropout)

        # Simpler attention pooling to reduce parameters
        self.attention = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 128),
            nn.Tanh(),
            nn.Dropout(dropout * 0.5),
            nn.Linear(128, 1)
        )

        # Simpler classifier to reduce overfitting
        self.classifier = nn.Sequential(
            nn.Linear(self.bert.config.hidden_size, 256),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(256, num_labels)
        )

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)

        # Get all hidden states
        hidden_states = outputs.last_hidden_state  # [batch_size, seq_len, hidden_size]

        # Attention pooling
        attention_weights = self.attention(hidden_states)  # [batch_size, seq_len, 1]
        attention_weights = torch.softmax(attention_weights, dim=1)

        # Weighted sum
        pooled_output = torch.sum(attention_weights * hidden_states, dim=1)  # [batch_size, hidden_size]

        pooled_output = self.dropout1(pooled_output)
        logits = self.classifier(pooled_output)

        return logits

def train_epoch(model, dataloader, optimizer, scheduler, criterion, device):
    model.train()
    total_loss = 0
    all_preds = []
    all_labels = []

    for batch in dataloader:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        optimizer.zero_grad()
        outputs = model(input_ids, attention_mask)
        loss = criterion(outputs, labels)
        loss.backward()

        # Gradient clipping (more aggressive)
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)

        optimizer.step()
        scheduler.step()

        total_loss += loss.item()
        preds = torch.argmax(outputs, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    acc = accuracy_score(all_labels, all_preds)

    return avg_loss, f1, acc

def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in dataloader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)

            outputs = model(input_ids, attention_mask)
            loss = criterion(outputs, labels)

            total_loss += loss.item()
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    avg_loss = total_loss / len(dataloader)
    f1 = f1_score(all_labels, all_preds, average='weighted')
    acc = accuracy_score(all_labels, all_preds)

    return avg_loss, f1, acc, all_preds

class EarlyStopping:
    def __init__(self, patience=3, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score):
        if self.best_score is None:
            self.best_score = score
        elif score < self.best_score + self.min_delta:
            self.counter += 1
            if self.counter >= self.patience:
                self.early_stop = True
        else:
            self.best_score = score
            self.counter = 0

def train_model(model_name, train_texts, train_labels, val_texts, val_labels):
    tokenizer = AutoTokenizer.from_pretrained(model_name)

    train_dataset = ArabicDataset(train_texts, train_labels, tokenizer, CONFIG['max_length'], augment=True)
    val_dataset = ArabicDataset(val_texts, val_labels, tokenizer, CONFIG['max_length'], augment=False)

    train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'], shuffle=True, num_workers=0)
    val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)

    model = ImprovedBertClassifier(model_name, num_labels=2, dropout=CONFIG['dropout']).to(device)
    optimizer = AdamW(model.parameters(), lr=CONFIG['learning_rate'], weight_decay=CONFIG['weight_decay'])
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=CONFIG['warmup_steps'],
        num_training_steps=len(train_loader) * CONFIG['num_epochs']
    )

    criterion = nn.CrossEntropyLoss(label_smoothing=CONFIG['label_smoothing'])
    early_stopping = EarlyStopping(patience=CONFIG['early_stopping_patience'], min_delta=0.0001)

    best_val_f1 = 0
    best_model_state = None

    for epoch in range(CONFIG['num_epochs']):
        train_loss, train_f1, train_acc = train_epoch(model, train_loader, optimizer, scheduler, criterion, device)

        val_loss, val_f1, val_acc, _ = validate(model, val_loader, criterion, device)

        print(
            f"Epoch {epoch+1}/{CONFIG['num_epochs']} \n"
            f"  Train:: Loss: {train_loss:.4f}, F1: {train_f1:.4f}, Acc: {train_acc:.4f}"
            f" | Val:: Loss: {val_loss:.4f}, F1: {val_f1:.4f}, Acc: {val_acc:.4f}"
        )

        if val_f1 > best_val_f1:
            best_val_f1 = val_f1
            best_model_state = model.state_dict().copy()

        early_stopping(val_f1)
        if early_stopping.early_stop:
          break

    if best_model_state is not None:
        model.load_state_dict(best_model_state)

    print(f"\nBest Validation F1: {best_val_f1:.4f}\n")

    return model, tokenizer, best_val_f1

def predict(model, test_loader, device):
    model.eval()
    predictions = []

    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)

            outputs = model(input_ids, attention_mask)
            preds = torch.argmax(outputs, dim=1)
            predictions.extend(preds.cpu().numpy())

    return predictions

df = pd.read_csv('ground_truth.csv', encoding='utf-8')
df = df.drop(columns=['id'])

label_map = {"human": 0, "machine": 1}
df["label"] = df["label"].map(label_map).astype(int)

cleaned_texts = df["content"].astype(str).apply(clean_text).tolist()
labels = df['label'].values

# Stratified train/validation split
train_texts, val_texts, train_labels, val_labels = train_test_split(
    cleaned_texts, labels,
    test_size=0.2,
    random_state=42,
    stratify=labels
)

# Train single model
model_name = CONFIG['model_name']
model, tokenizer, val_f1 = train_model(model_name, train_texts, train_labels, val_texts, val_labels)

# Final validation with detailed metrics
val_dataset = ArabicDataset(val_texts, val_labels, tokenizer, CONFIG['max_length'], augment=False)
val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)

val_preds = predict(model, val_loader, device)

val_f1_weighted = f1_score(val_labels, val_preds, average='weighted')

print(f"\nClassification Report:")
print(classification_report(val_labels, val_preds, target_names=['human', 'machine']))

print(f"Validation F1 = {val_f1_weighted:.4f}")

# Predict on test set
df_test = pd.read_csv('final_test_unlabeled.csv', encoding='utf-8')
test_ids = df_test["id"].values
cleaned_test = df_test["content"].astype(str).apply(clean_text).tolist()

test_dataset = ArabicDataset(cleaned_test, None, tokenizer, CONFIG['max_length'], augment=False)
test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'], shuffle=False, num_workers=0)

test_predictions = predict(model, test_loader, device)

label_map_r = {0: "human", 1: "machine"}
pred_labels = [label_map_r[p] for p in test_predictions]

submission_df = pd.DataFrame({
    "label": pred_labels
})

submission_df.to_csv("predictions_test.csv", index=False)
print(f"Test predictions distribution: {pd.Series(pred_labels).value_counts().to_dict()}")

